
[{"content":"","date":"3 July 2025","externalUrl":null,"permalink":"/","section":"BeyondElastic","summary":"","title":"BeyondElastic","type":"page"},{"content":"","date":"3 July 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"23 April 2025","externalUrl":null,"permalink":"/tags/agents/","section":"Tags","summary":"","title":"Agents","type":"tags"},{"content":"","date":"23 April 2025","externalUrl":null,"permalink":"/categories/ai/","section":"Categories","summary":"","title":"AI","type":"categories"},{"content":"","date":"23 April 2025","externalUrl":null,"permalink":"/tags/azure/","section":"Tags","summary":"","title":"Azure","type":"tags"},{"content":"","date":"23 April 2025","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"23 April 2025","externalUrl":null,"permalink":"/tags/function-calling/","section":"Tags","summary":"","title":"Function Calling","type":"tags"},{"content":"","date":"23 April 2025","externalUrl":null,"permalink":"/tags/plugins/","section":"Tags","summary":"","title":"Plugins","type":"tags"},{"content":"","date":"23 April 2025","externalUrl":null,"permalink":"/tags/semantic-kernel/","section":"Tags","summary":"","title":"Semantic Kernel","type":"tags"},{"content":" Intro # These days, LLMs are not good enough anymore. We build agentic applications that perform complex tasks with up-to-date data and external systems. This is why we need multiple specialized AI Agents that can access all sorts of APIs and data sources to provide additional context to our LLMs. In this blog post, I want to take a closer look at Semantic Kernel Function Calling \u0026amp; Plugins and how it can help in such scenarios.\nSemantic Kernel # Semantic Kernel is a lightweight multiagent orchestration framework. It allows us to build enterprise-grade multiagent AI apps by combining and integrating multiple AI models and systems. In one of my last blog posts, I explained how to use Semantic Kernel to have multiple AI agents interacting with each other via a group chat. Today we are going to look at extensibility!\nFunction Calling # What is function calling? Function calling is a feature that enables AI models to request the execution of specific functions, allowing them to perform actions based on user inputs. Semantic Kernel then marshals the request to the appropriate function in your codebase and returns the results to the LLM, so the LLM can generate a final response. This capability is particularly useful for enabling AI to interact with and invoke your APIs. It is a native feature in most of the latest large language models (LLMs), facilitating planning and execution of tasks.\nPlugins # Aren\u0026rsquo;t plugins the same? Yes and no. At a high level, a plugin is a collection of functions that can be made available to AI applications and services. These functions can be orchestrated by an AI application to fulfill user requests. Behind the scenes, Semantic Kernel utilizes function calling to enable this process. Additionally, plugins support dependency injection, allowing essential services such as database connections or HTTP clients to be injected into the plugin\u0026rsquo;s constructor.\nIf you do some research about Semantic Kernel plugins and function calling, you might come across skills. What used to be skills became plugins to align with the OpenAI plugin specification. You can read more about it in the following post:\nSkills to plugins: fully embracing the OpenAI plugin spec in Semantic Kernel There are three main ways to get plugins into Semantic Kernel:\nUsing native code Using an OpenAPI specification Using an MCP Server The latter two are programming languages and platform-agnostic. However, we will use the native code option as it is the easiest to start with.\nEnough theory, let\u0026rsquo;s do some coding!\nCoding # For this demonstration, I want to write a simple Semantic Kernel AI app that uses a native code plugin. For those who know me, I am a huge üèÄ Basketball fan. In my spare time, I coach a children\u0026rsquo;s team in my hometown, and I still play Basketball myself. The Basketball EuroLeague is currently in playoff mode, and I like to follow the latest game results. That\u0026rsquo;s why I am going to write a plugin that uses the EuroLeague API to provide the latest game results to my LLM.\nWriting our Semantic Kernel plugin # When writing the plugin, it is important to give special attention to function naming. It is recommended to use snake case for function names and properties, as most models are trained with Python for function calling. The model needs to understand its intent and parameters. If needed, you can, and in some cases, you should add a description to it, but it will increase the token usage. In our case, it is a simple function with only one input parameter and a self-explanatory name, get_latest_euroleague_game_results. Nevertheless, I have annotated the season input parameter for the model to know what is expected.\nAll we really need to do is create a class and annotate its methods with the @kernel_function attribute. This way, Semantic Kernel knows that this is a function that can be called by our LLM. Note that the helper function xml_to_dict is not annotated and won\u0026rsquo;t be sent to the model.\nimport xml.etree.ElementTree as ET import httpx from typing import Any, Dict, Annotated from datetime import datetime from semantic_kernel.functions import kernel_function # This plugin fetches EuroLeague game results and processes them. class EuroleaguePlugin: # This function converts an XML element and its children to a dictionary. def xml_to_dict(self, element: ET.Element) -\u0026gt; Any: \u0026#34;\u0026#34;\u0026#34;Recursively converts an XML element and its children to a dictionary.\u0026#34;\u0026#34;\u0026#34; node = {} if element.attrib: node.update(element.attrib) children = list(element) if children: for child in children: child_dict = self.xml_to_dict(child) if child.tag not in node: node[child.tag] = [] node[child.tag].append(child_dict) else: node = element.text or \u0026#34;\u0026#34; return node # This function fetches the latest six EuroLeague game results for a given season. @kernel_function async def get_latest_euroleague_game_results(self, season: Annotated[str, \u0026#34;The year of the season, e.g. 2024 for season 2024/25\u0026#34;]) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;Fetches EuroLeague game results and returns only the last 6 games by date.\u0026#34;\u0026#34;\u0026#34; url = \u0026#34;https://api-live.euroleague.net/v1/results\u0026#34; params = {\u0026#34;season_code\u0026#34;: \u0026#34;E\u0026#34; + season} headers = {\u0026#34;Accept\u0026#34;: \u0026#34;application/xml\u0026#34;} try: async with httpx.AsyncClient() as client: response = await client.get(url, params=params, headers=headers) response.raise_for_status() root = ET.fromstring(response.text) data = self.xml_to_dict(root) # Extract the games from the parsed XML data games = data.get(\u0026#34;game\u0026#34;, []) # Parse and sort games by date def parse_date(game): # The date is a list, so get the first element date_val = game.get(\u0026#34;date\u0026#34;, [\u0026#34;\u0026#34;]) if isinstance(date_val, list): date_str = date_val[0] else: date_str = date_val try: return datetime.strptime(date_str, \u0026#34;%b %d, %Y\u0026#34;) except Exception: return datetime.min # Sort the games by date in descending order games_sorted = sorted(games, key=parse_date, reverse=True) last_6_games = games_sorted[:6] # Replace the games in the data dict with only the last 6 data[\u0026#34;game\u0026#34;] = last_6_games return data # Exception handling except Exception as e: print(f\u0026#34;Exception when making direct request: {e}\u0026#34;) return {} So far, so good. We have a plugin. What is doing exactly? It is getting the latest six Euroleague game results of a specific season. The helper function xml_to_dict converts the XML response from the EuroLeague API to a Python dictionary, so it is easier to process for the model. We then sort the data in descending order by date and truncate it, leaving only the last six games.\nWriting our Semantic Kernel AI app # Now that we have a plugin, let\u0026rsquo;s write our AI app that will use the plugin. This script uses Azure OpenAI chat completion, adds our Euroleague plugin to the kernel, and creates a chat history to interact with the assistant.\nAdditionally, we define the AzureChatPromptExecutionSettings to configure how prompts are executed when interacting with Azure OpenAI. It allows you to set options such as function calling behavior, temperature, and other model parameters. In our case, we set the FunctionChoiceBehavior to auto. This setting lets the model automatically select the most relevant function based on the user\u0026rsquo;s input.\nLastly, the script sends a user message requesting the latest Euroleague game results and prints the AI\u0026rsquo;s response.\nimport asyncio import os import euroleague_plugin from dotenv import load_dotenv from semantic_kernel import Kernel from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, AzureChatPromptExecutionSettings from semantic_kernel.connectors.ai import FunctionChoiceBehavior from semantic_kernel.contents import ChatHistory load_dotenv() # Azure OpenAI config (set your environment variables) AZURE_OPENAI_KEY = os.getenv(\u0026#34;AZURE_OPENAI_KEY\u0026#34;) AZURE_OPENAI_ENDPOINT = os.getenv(\u0026#34;AZURE_OPENAI_ENDPOINT\u0026#34;) AZURE_OPENAI_DEPLOYMENT = os.getenv(\u0026#34;AZURE_OPENAI_DEPLOYMENT\u0026#34;) async def main(): # Initialize the kernel kernel = Kernel() # Add Azure OpenAI chat completion chat_completion = AzureChatCompletion( deployment_name=AZURE_OPENAI_DEPLOYMENT, api_key=AZURE_OPENAI_KEY, endpoint=AZURE_OPENAI_ENDPOINT, ) kernel.add_service(chat_completion) # Add a plugin (the EuroleaguePlugin class is defined above) kernel.add_plugin( euroleague_plugin.EuroleaguePlugin(), plugin_name=\u0026#34;Euroleague\u0026#34; ) # Enable planning execution_settings = AzureChatPromptExecutionSettings() execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto() # Create a history of the conversation history = ChatHistory() history.add_user_message(\u0026#34;Please give me the latest Euroleague game results for the 2024 season.\u0026#34;) # Get the response from the AI result = await chat_completion.get_chat_message_content( chat_history=history, settings=execution_settings, kernel=kernel, ) # Print the results print(\u0026#34;Assistant \u0026gt; \u0026#34; + str(result)) # Add the message from the agent to the chat history history.add_message(result) if __name__ == \u0026#34;__main__\u0026#34;: asyncio.run(main()) Let\u0026rsquo;s see if our code works as expected and if our AI app will indeed use the plugin to fetch the latest game results from the EuroLeague API.\nRunning our Semantic Kernel AI app # Note: We haven\u0026rsquo;t covered how to create the Azure OpenAI Service or deployment. If you need some guidance, you can follow the documentation here. Make sure you have an up-to-date model (e.g., gpt-4o) deployed and a local .env file with the endpoint, key and deployment name filled out.\nI will simply execute the app in my terminal and see if we get the expected response back.\n‚ûú python app3.py Assistant \u0026gt; Here are the latest Euroleague game results for the 2024 season: 1. **April 24, 2025** - **Panathinaikos Aktor Athens** vs. **Anadolu Efes Istanbul**: 76 - 79 - **Fenerbahce Beko Istanbul** vs. **Paris Basketball**: 89 - 72 2. **April 23, 2025** - **Olympiacos Piraeus** vs. **Real Madrid**: 84 - 72 - **AS Monaco** vs. **FC Barcelona**: 97 - 80 3. **April 22, 2025** - **Fenerbahce Beko Istanbul** vs. **Paris Basketball**: 83 - 78 - **Panathinaikos Aktor Athens** vs. **Anadolu Efes Istanbul**: 87 - 83 These games were part of the playoffs. Great! This is accurate and everything I wanted. The model provides me with the latest six-game results that happened just a couple of days ago. We can verify if the results are correct by checking the EuroLeague Game Center page.\nIf you want to check out the code, you can find it on my GitHub repo.\nSummary # This was a very straightforward example of how to use Semantic Kernel plugins to reach out to external systems to provide up-to-date data to our LLM. Encapsulating native code within a plugin is a simple method to equip an AI agent with capabilities that it doesn\u0026rsquo;t inherently support. This approach enables you to utilize your existing app development skills and code to enhance the functionality of your AI agents. Besides native code, MCP, and OpenAPI plugins, we can also make use of Retrieval Augmented Generation (RAG) patterns when it comes to data access and grounding. For example, we can build a data retrieval plugin using Azure AI Search as described here.\nPlenty of possibilities to make our AI agents smarter. I am especially excited about the MCP option, as I recently wrote an intro blog post to MCP, and I can\u0026rsquo;t wait to combine it with Semantic Kernel.\nSources # Semantic Kernel function calling Semantic Kernel plugins Skills to plugins: fully embracing the OpenAI plugin spec in Semantic Kernel Semantic Kernel plugins - native code Semantic Kernel plugins - OpenAPI specification Semantic Kernel plugins - MCP Server Snake case Semantic Kernel KernelFunctions Semantic Kernel ChatCompletion Semantic Kernel ChatHistory Semantic Kernel AzureChatPromptExecutionSettings Semantic Kernel FunctionChoiceBehavior Deploy Azure OpenAI via Bicep Semantic Kernel RAG plugin EuroLeague Game Center EuroLeague API ","date":"23 April 2025","externalUrl":null,"permalink":"/posts/skfc/","section":"Posts","summary":"","title":"Semantic Kernel Function Calling \u0026 Plugins","type":"posts"},{"content":"","date":"23 April 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"11 April 2025","externalUrl":null,"permalink":"/tags/copilot/","section":"Tags","summary":"","title":"Copilot","type":"tags"},{"content":"","date":"11 April 2025","externalUrl":null,"permalink":"/tags/github/","section":"Tags","summary":"","title":"GitHub","type":"tags"},{"content":" Intro # The Model Context Protocol (MCP) is trending! What is it? Let\u0026rsquo;s check it out. MCP is an open-source project launched in November 2024. It defines an open standard for AI applications to connect to their tools and data. I heard someone referring to it as the USB adapter for AI systems because it provides a very much needed standardization. Making your AI models smarter typically requires custom code, which can be challenging when it comes to scaling and maintaining the integrations. Most likely, your teams will end up writing the same integration multiple times for different AI use-cases. MCP allows for a \u0026ldquo;write once, work everywhere\u0026rdquo; approach, which will save development efforts.\nAdditionally, I recommend watching the John Savill\u0026rsquo;s \u0026ldquo;Model Context Protocol Overview - Why You Care!\u0026rdquo; video:\nOverview # In this blog post, I want to explain and demonstrate MCP with a simple but useful example. As everyone is currently talking about MCP and the project gains popularity, we can find more and more content about it online. For example, the collection of MCP Servers is growing by the minute. Just a couple of days ago, the public preview of Azure MCP Server was announced. As I spent most of my time in VS Code working with GitHub Copilot, I want to run the Azure MCP Server locally and use VS Code as the client that connects to it. But basics first!\nMCP Server # What is an MCP Server? MCP uses a client-server architecture. The MCP Server is where you build your connection to your APIs and resources. This is where the heavy lifting is done as you still have to write the integration. Nevertheless, the MCP Server then exposes your services as MCP Tools via the standardized Model Context Protocol and makes them available to your AI apps.\nThe MCP Servers can run local (stdio) or remote via HTTP+SSE(Server-Sent Events) transport layer. However, the remote implementation is still in early stages and is evolving fast. The recent specs already replacing HTTP+SSE with Streamable HTTP. I found a great blog post from Christian Posta explaining the change in more detail.\nUnderstanding MCP Recent Change Around HTTP+SSE If you want to host a remote MCP Server on Azure, there are already multiple blog posts and repos available that you can follow:\nHost remote MCP Servers on Azure App Service Host remote MCP Servers on Azure Container Apps Host remote MCP Servers on Azure Functions When we talk remote MCP Servers, authentication becomes very important, and I can recommend following Den Delimarsky\u0026rsquo;s blog posts and his Entra ID integration ideas. But be aware, this is also evolving fast and might be irrelevant next week.\nUsing Microsoft Entra ID To Authenticate With MCP Servers Via Sessions A list of example MCP Servers can be found here, and a quickstart on how to write your own server here. In this blog post, we are using a local MCP Server for the ease of use.\nMCP Host / Client # The MCP Host can be your AI app or LLM based coding client. It is responsible for generating tasks or queries, but does not directly interact with data sources or APIs. The MCP Host can create and manage multiple client instances.\nThe MCP Client acts as an intermediary between the MCP Host and the MCP Server. It manages connections, handles communication, discovers and executes tools, and facilitates resource access. This ensures that the AI model can perform its tasks efficiently and effectively by leveraging the capabilities of the MCP Servers.\nIn summary, the MCP Client acts as a bridge between the AI model and the MCP Servers.\nflowchart LR subgraph \"MCP Host\" direction LR A[MCP Client] B[MCP Server] C[MCP Client] D[MCP Server] G[MCP Client] end E[Resource] F[API] I[API] H[Remote MCP Server] A--\u003eB B\u003c--\u003eE C--\u003eD D\u003c--\u003eF G--\u003eH H\u003c--\u003eI A list of applications that support MCP can be found here and a quickstart on how to build your own client here. Additionally, you can find more about the MCP architecture in the documentation and from the latest specifications as of writing this blog post.\nMCP Capabilities # What are the capabilities that can be exposed by MCP Servers and used by MCP Clients?\nMCP Tools: Functions that can be invoked by your LLMs. This allows for automation and extensibility into the outside world. MCP Resources: Data that can be accessed by your LLMs. This allows for providing data such as files, databases, log files etc\u0026hellip; as context to your LLMs. MCP Prompts: Reusable prompt templates that can be used by your LLMs. This allows to standardize the LLM interactions. MCP Sampling: Allows the MCP Server to request completions from the client. This is very early stages and the client support is still lacking. However, this can become very powerful as it enables bidirectional communication. Implementation # Now that we have covered the basics, let\u0026rsquo;s start with the implementation. As mentioned above, I want to use the Azure MCP Server with VS Code and GitHub Copilot.\nPrerequisites # If you want to follow along, make sure you have the following prerequisites in place:\nAzure Subscription Azure CLI GitHub account and a GitHub Copilot free plan or higher VS Code with GitHub Copilot and Agent Mode enabled Make sure VS Code GitHub Copilot is signed in with your GitHub account, agent mode is enabled and selected on the bottom of the chat window.\nConfiguration # First, open a terminal in VS Code and sign in to our Azure Subscription with the az logincommand. Create an empty folder and create a .vscode folder inside the first folder. Create a mcp.json file within the .vscode folder and add the following content to it. { \u0026#34;servers\u0026#34;: { \u0026#34;Azure MCP Server\u0026#34;: { \u0026#34;command\u0026#34;: \u0026#34;npx\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;-y\u0026#34;, \u0026#34;@azure/mcp\u0026#34;, \u0026#34;server\u0026#34;, \u0026#34;start\u0026#34; ] } } } Note: I experienced some processor architecture related issues when using \u0026ldquo;@azure/mcp@latest\u0026rdquo; package as suggested by the blog post or the repo. I am running on a MacBook with M2 processor, and it was not finding the mcp-darwin-arm64 package. Nevertheless, after removing the @latest it was working, and the version is also identical (0.0.10 at the time of writing this blog post).\nAfter saving the mcp.json you should see a little \u0026ldquo;Start\u0026rdquo; button appearing over \u0026ldquo;Azure MCP Server\u0026rdquo;. Press it! This will start the Azure MCP Server, and it will discover the MCP Tools provided by it. Now we should see the discovered tools in our Agent mode chat window. We can inspect the available tools by clicking on the tools icon. So far so good, let\u0026rsquo;s try the MCP Tools.\nExecution # Now that we have everything in place, let\u0026rsquo;s ask our Copilot Agent something about Azure, e.g. \u0026ldquo;list all my Azure Resource Groups\u0026rdquo;. It will automatically realize that it has MCP Tools available that can help here. The Agent will also recognize that it first has to get the Azure Subscription by running the \u0026ldquo;azmcp-subscription-list\u0026rdquo; tool. We can get more details about the tool by clicking on \u0026ldquo;\u0026gt; Run\u0026rdquo;.\nAfter approving the tool execution by clicking \u0026ldquo;Continue\u0026rdquo; it will run the first tool and ask to execute the second tool to list my Resource Groups. As soon as we approve the next step, it will show us a list with our Resource Groups.\nAwesome, we can now use the MCP Tools provided by our Azure MCP Server to execute azd commands directly or query logs and more. Combine this with the Azure extension for GitHub Copilot and you have a powerful Azure toolset directly in VS Code GitHub Copilot.\nSummary # To summarize, MCP provides a very much needed standardization in the ever-growing AI Agent space. We can already see that multiple vendors adopt it and provide MCP Servers for their solutions. In addition, the amount of community provided MCP Servers is growing by the minute. In an ideal case, you don\u0026rsquo;t have to write your own MCP Server, you just look it up in an MCP Server registry.\nHere is a list of popular MCP Server registries:\nhttps://mcp.so/servers https://glama.ai/mcp/servers ‚Äã https://smithery.ai/ https://www.pulsemcp.com/servers If you wonder, is it is possible to use Azure AI Agent Service with MCP, it is. There is a great blog post available here. It describes how to use Azure AI Agents as MCP Tools. Which might sounds strange in the first moment as it works the other way around. In this scenario, the MCP Server uses the agent or multiple agents as tools instead of the agent querying the MCP Server for exposed tools. Which makes a lot of sense if you think about the many ootb tools and Azure integrations that are available with the Azure AI Agent Service.\nBesides MCP, there is another standard that was just recently announced by Google. The A2A protocol, which caters more to the agent to agent collaboration across platforms. It is definitely worth to follow both projects and see how they evolve. There is no doubt the future of agentic apps looks bright.\nResources # Azure MCP Server MCP architecture documentation MCP Server quickstart MCP Server examples MCP Client quickstart MCP Client examples MCP specifications 26.03.2025 MCP Tools MCP Resources MCP Prompts MCP Sampling Understanding MCP Recent Change Around HTTP+SSE Host remote MCP Servers on Azure App Service Host remote MCP Servers on Azure Container Apps Host remote MCP Servers on Azure Functions Using Microsoft Entra ID To Authenticate With MCP Servers Via Sessions Azure Subscription Azure CLI documentation GitHub account VS Code GitHub Copilot GitHub Copilot Agent Mode Azure Developer CLI GitHub Copilot Azure extension Azure AI Agent Service MCP with Azure AI Agent Service A2A protocol ","date":"11 April 2025","externalUrl":null,"permalink":"/posts/mcp/","section":"Posts","summary":"","title":"Intro to Model Context Protocol","type":"posts"},{"content":"","date":"11 April 2025","externalUrl":null,"permalink":"/tags/mcp/","section":"Tags","summary":"","title":"MCP","type":"tags"},{"content":"","date":"11 April 2025","externalUrl":null,"permalink":"/tags/vs-code/","section":"Tags","summary":"","title":"VS Code","type":"tags"},{"content":"","date":"10 April 2025","externalUrl":null,"permalink":"/tags/foundry/","section":"Tags","summary":"","title":"Foundry","type":"tags"},{"content":" Intro # In my last post, I have covered Azure AI Agent Service and how it can be used to easily build and run AI agents on Azure. This time, we are going to look at Semantic Kernel as a framework to build, orchestrate and deploy AI agents or multi-agent applications. Semantic Kernel is an open-source development kit by Microsoft that offers a unified framework with a plugin-based architecture for easier integration and reduced complexity. It serves as efficient middleware, enabling fast development of enterprise-grade solutions by combining prompts with existing APIs.\nThe Semantic Kernel SDK is available for C#, Python and Java. More details can be found on the official GitHub repository and the official Microsoft documentation.\nIn this blog post, we are going to combine the Azure AI Agent Service with Semantic Kernel to build a multi-agent AI application with group chat functionality.\nBackground # If you are new to the topic, it can be a bit confusing as there are many options to choose from. Which framework should I use? AutoGen or Semantic Kernel? Which API should I use the Chat Completions API, the Assistants API or the Azure AI Agent Service? Well, famous last words \u0026ldquo;it depends\u0026rdquo; and \u0026ldquo;things are evolving fast\u0026rdquo;.\nThe framework discussion is manly driven by whether you need enterprise-grade support or not. If that is a yes, you should look towards Semantic Kernel. If you are still in the ideation/testing phase, and you need the latest and greatest functionality, take a look at AutoGen. Both teams are working on strategic convergence and integrations between both frameworks, as you can read in the following blog posts:\nMicrosoft‚Äôs Agentic AI Frameworks: AutoGen and Semantic Kernel Semantic Kernel Roadmap H1 2025: Accelerating Agents, Processes, and Integration AutoGen and Semantic Kernel, Part 2 In terms of API, the Chat Completions API is lightweight and stateless and can be a good fit for simple tasks. The Assistants API is stateful (managing conversation history) and can be a good fit for more complex scenarios. Azure AI Agent Service delivers all the functionality of the Assistants API plus flexible model choice, out of the box tools, tracing and more.\nRequirements # As an execution engine, we will use the Azure AI Agent Service. We are not going to cover the infrastructure requirements in this blog post. Nevertheless, if you want to get started quickly, simply deploy this bicep template for an standard Azure AI Agent deployment.\nPrepare local dev environment # For our local development environment, we need to install the following packages:\npip install python-dotenv azure-identity semantic-kernel[azure] Next, we will use a local .env file to specify some variables to connect to our Azure AI Foundry Project.\nAZURE_AI_AGENT_PROJECT_CONNECTION_STRING=\u0026#34;your_project_connection_string\u0026#34; AZURE_AI_AGENT_MODEL_DEPLOYMENT_NAME=\u0026#34;your_model_deployment\u0026#34; Coding # My goal is to create multiple AI agents that act as Basketball coaches. A Head Coach and an Assistant Coach that will exchange ideas and come up with a game plan for a specific game situation.\nFor this blog, we will keep it very simple and don\u0026rsquo;t play too much with plugins or extensions. We just want to create a group chat with specialized agents.\nBuilding the mulit-agent app # As always we need to add some references first. This time we are going to import the asyncio model as we need to execute the main function asynchronously. This allows the program to perform non-blocking operations, such as interacting with Azure AI services, creating agents, and managing the group chat. Additionally, we are going to import some components from the Semantic Kernel library. More about these Semantic Kernel classes can be found later in the text.\n# add references import asyncio from dotenv import load_dotenv from azure.identity.aio import DefaultAzureCredential from semantic_kernel.agents import AzureAIAgent, AzureAIAgentSettings, AgentGroupChat from semantic_kernel.agents.strategies import TerminationStrategy, SequentialSelectionStrategy from semantic_kernel.contents.utils.author_role import AuthorRole Now we are going to load the environment variables from the .env file and define the agent names and instructions as well as the task they should work on.\n# get configuration settings load_dotenv() # agent instructions HEAD_COACH = \u0026#34;HeadCoach\u0026#34; HEAD_COACH_INSTRUCTIONS = \u0026#34;\u0026#34;\u0026#34; You are a Basketball Head Coach that knows everything about offensive plays and strategies. You respond to specific game situations with advice on how to change the game plan. You can ask for more information about the game situation if needed. For offensive plays and strategies, you will decide on the strategy yourself. If the game situation demands a change for defensive plays and strategies, you will ask your Assistant Coach for advice. You will use the advice given by the Assistant Coach regarding defensive adjustments and your own decision for offensive adjustments to create the final game plan. RULES: - Use the instructions provided. - Prepend your response with this text: \u0026#34;head_coach \u0026gt; \u0026#34; - Do not directly answer the question if it is related to defensive strategies. Instead, ask your Assistant Coach for advice. - Do not use the words \u0026#34;final game plan\u0026#34; unless you have created a final game plan according to the instructions. - Add \u0026#34;final game plan\u0026#34; to the end of your response if you have created a final game plan according to the instructions. \u0026#34;\u0026#34;\u0026#34; ASSISTANT_COACH = \u0026#34;AssistantCoach\u0026#34; ASSISTANT_COACH_INSTRUCTIONS = \u0026#34;\u0026#34;\u0026#34; You are a Basketball Assistant Coach that knows defensive plays and strategies. You give advice to your Head Coach for specific game situations that require defensive adjustment. RULES: - Use the instructions provided. - Prepend your response with this text: \u0026#34;assistant_coach \u0026gt; \u0026#34; - You are not allowed to give advice on offensive plays and strategies. - You don\u0026#39;t decide the final game strategy and plan, you only give advice to the Head Coach. - Your advice should be clear and concise and should not include any unnecessary information. \u0026#34;\u0026#34;\u0026#34; # agent task TASK = \u0026#34;Could you please give me advice on how to change the game strategy for the next quarter? We are playing zone defense, and the other team just scored 10 points in a row. We need to change our strategy to stop them. What should we do?\u0026#34; So far, so good. We will now start with our main function. We retrieve the configuration settings with the AzureAIAgentSettings.create method and use the DefaultAzureCredential class to authenticate against our Azure Services, and lastly, we create a client for interacting with the Azure AI agent service.\nasync def main(): ai_agent_settings = AzureAIAgentSettings.create() async with ( DefaultAzureCredential(exclude_environment_credential=True, exclude_managed_identity_credential=True) as creds, AzureAIAgent.create_client(credential=creds) as client, ): The next step is to create our agents on the Azure AI Agent Service and wrap them into Semantic Kernel agents using the AzureAIAgent class.\n# create the head-coach agent on the Azure AI agent service headcoach_agent_definition = await client.agents.create_agent( model=ai_agent_settings.model_deployment_name, name=HEAD_COACH, instructions=HEAD_COACH_INSTRUCTIONS, ) # create a Semantic Kernel agent for the Azure AI head-coach agen agent_headcoach = AzureAIAgent( client=client, definition=headcoach_agent_definition, ) # create the assistant coach agent on the Azure AI agent service assistantcoach_agent_definition = await client.agents.create_agent( model=ai_agent_settings.model_deployment_name, name=ASSISTANT_COACH, instructions=ASSISTANT_COACH_INSTRUCTIONS, ) # create a Semantic Kernel agent for the assistant coach Azure AI agent agent_assistantcoach = AzureAIAgent( client=client, definition=assistantcoach_agent_definition, ) This is where the fun part begins. We are initializing a group chat via the AgentGroupChat class and adding our agents to it. We need to define who comes next and when the group chat should end. To do so, we define a termination and selection strategy. Additionally, we define which agent is contributing to the termination strategy. In our case, only the Head Coach is in charge. Furthermore, we are defining a maximum of 4 iterations until the chat will be terminated.\n# add the agents to a group chat with a custom termination and selection strategy chat = AgentGroupChat( agents=[agent_headcoach, agent_assistantcoach], termination_strategy=ApprovalTerminationStrategy( agents=[agent_headcoach], maximum_iterations=4, automatic_reset=True ), selection_strategy=SelectionStrategy(agents=[agent_headcoach,agent_assistantcoach]), ) In this section, we handle the execution of the group chat, including adding the task, invoking the chat, and performing cleanup operations. The AuthorRole.USER constant is used to explicitly identify the role of the message sender as the user to ensure clarity in the conversation flow.\ntry: # add the task as a message to the group chat await chat.add_chat_message(message=TASK) print(f\u0026#34;# {AuthorRole.USER}: \u0026#39;{TASK}\u0026#39;\u0026#34;) # invoke the chat async for content in chat.invoke(): print(f\u0026#34;# {content.role} - {content.name or \u0026#39;*\u0026#39;}: \u0026#39;{content.content}\u0026#39;\u0026#34;) finally: # cleanup and delete the agents print(\u0026#34;--chat ended--\u0026#34;) await chat.reset() await client.agents.delete_agent(agent_headcoach.id) await client.agents.delete_agent(agent_assistantcoach.id) Almost at the end, we just need to add two classes for the termination and selection function that we have used in the group chat definition. As we defined in the Head Coach agent instructions, as soon as the final game plan is ready, it should add \u0026ldquo;final game plan\u0026rdquo; to its message. We are checking if the last message in the history contains this phrase, and we will terminate the group chat.\n# class of termination strategy class ApprovalTerminationStrategy(TerminationStrategy): \u0026#34;\u0026#34;\u0026#34;A strategy for determining when an agent should terminate.\u0026#34;\u0026#34;\u0026#34; async def should_agent_terminate(self, agent, history): \u0026#34;\u0026#34;\u0026#34;Check if the agent should terminate.\u0026#34;\u0026#34;\u0026#34; return \u0026#34;final game plan\u0026#34; in history[-1].content.lower() The second class adds a selection function that defines which agent should take the next turn in the chat. If the last message comes from the User or the Assistant Coach, it is the Head Coaches turn.\n# class for selection strategy class SelectionStrategy(SequentialSelectionStrategy): \u0026#34;\u0026#34;\u0026#34;A strategy for determining which agent should take the next turn in the chat.\u0026#34;\u0026#34;\u0026#34; # select the next agent that should take the next turn in the chat async def select_agent(self, agents, history): \u0026#34;\u0026#34;\u0026#34;\u0026#34;Check which agent should take the next turn in the chat.\u0026#34;\u0026#34;\u0026#34; # the Head Coach should go after the User or the Assistant Coach if (history[-1].name == ASSISTANT_COACH or history[-1].role == AuthorRole.USER): agent_name = HEAD_COACH return next((agent for agent in agents if agent.name == agent_name), None) # otherwise it is the Assistant Coach\u0026#39;s turn return next((agent for agent in agents if agent.name == ASSISTANT_COACH), None) Lastly, we define the entry point for our app, and that it is executed as an asynchronous coroutine.\nif __name__ == \u0026#34;__main__\u0026#34;: asyncio.run(main()) Running the mulit-agent app # Ok, we should have something to play and test with. The finale code can be found on my GitHub repo here.\nLet\u0026rsquo;s see if we can get a proper game plan from our coaching staff.\n‚ûú python app.py # AuthorRole.USER: \u0026#39;Could you please give me advice on how to change the game strategy for the next quarter? We are playing zone defense and the other team just scored 10 points in a row. We need to change our strategy to stop them. What should we do?\u0026#39; # AuthorRole.ASSISTANT - HeadCoach: \u0026#39;head_coach \u0026gt; I\u0026#39;ll need to consult with the Assistant Coach about defensive adjustments since that\u0026#39;s not my area of expertise. Assistant Coach, what adjustments do you recommend for our zone defense to stop the opposing team who has just scored 10 points in a row? In terms of our offensive strategy, we should focus on enhancing our ball movement and executing quick passes to exploit the gaps in their defense. Let\u0026#39;s emphasize perimeter shooting and look for opportunities to drive to the basket, ensuring we spread the floor to create space. Please provide your defensive advice, and I\u0026#39;ll integrate that with our offensive strategy for the necessary adjustments.\u0026#39; # AuthorRole.ASSISTANT - AssistantCoach: \u0026#39;assistant_coach \u0026gt; Consider switching to a man-to-man defense to apply more pressure on their shooters and disrupt their rhythm. This will help limit their easy scoring opportunities and force them into more contested shots. Ensure our players communicate effectively and switch on screens. Additionally, encourage tighter closeouts on shooters to contest their shots and deny open looks. If they continue to score, we could also implement a trap to force turnovers and get out in transition.\u0026#39; # AuthorRole.ASSISTANT - HeadCoach: \u0026#39;head_coach \u0026gt; Thank you, Assistant Coach. Based on your advice, we\u0026#39;ll switch to a man-to-man defense to apply pressure and limit their scoring opportunities. We\u0026#39;ll focus on strong communication and switching on screens, as well as tighter closeouts on shooters. On the offensive side, we\u0026#39;ll continue to enhance our ball movement, emphasizing quick passes and prioritizing perimeter shooting, alongside drive opportunities. This blend of a more aggressive defensive approach and a fluid offensive strategy should help us regain control of the game. Now, let\u0026#39;s put this all together: we\u0026#39;ll implement a man-to-man defense while enhancing our offensive ball movement and exploiting gaps in their defense. final game plan\u0026#39; --chat ended-- Nice! Thanks Coaching staff! That indeed sounds like a plan to win the game in the end.\nIn Azure AI Foundry, we can see that the corresponding Azure AI Agents are getting created during the runtime and cleaned up afterward.\nSummary # This was a very simple example, but it shows how multiple agents can have different expertise and exchange ideas or knowledge about a specific topic via the Semantic Kernel group chat. Additionally, we can facilitate a structured conversation flow that allows the agents to efficiently collaborate and work on user provided tasks. Imagine that these agents would have access to different tools or knowledge sources to make them specialists for a specific task. We already looked at how to add tools (Code Interpreter Tool) to Azure AI Agents in my last blog post. The same approach can be used in combination with Semantic Kernel to make our agents even smarter.\nSources # Semantic Kernel Microsoft documentation Semantic Kernel GitHub repository Microsoft‚Äôs Agentic AI Frameworks: AutoGen and Semantic Kernel Semantic Kernel Roadmap H1 2025: Accelerating Agents, Processes, and Integration AutoGen and Semantic Kernel, Part 2 Azure AI Agent standard setup bicep template Intro to Azure AI Agent Service Microsoft Learn AI Agent Fundamentals Introducing enterprise multi-agent support in Semantic Kernel Semantic Kernel Agents are now Generally Available ","date":"10 April 2025","externalUrl":null,"permalink":"/posts/multi-agent/","section":"Posts","summary":"","title":"Intro to Semantic Kernel and Multi-Agent AI Apps","type":"posts"},{"content":"","date":"10 April 2025","externalUrl":null,"permalink":"/tags/multi-agent/","section":"Tags","summary":"","title":"Multi-Agent","type":"tags"},{"content":" Intro # AI Agents are in talks, and we are going to take a look at the Azure AI Agent Service. Why should you care? Because AI Agents are the next evolution of AI-driven applications, and they will help you to automate and execute more complicated multistep tasks completely autonomously or with a human in a loop. Here are two blog posts that are worth reading to set the scene:\nAI agents ‚Äî what they are, and how they‚Äôll change the way we work Introducing Azure AI Agent Service In this blog post, I will give you a short overview about what the Azure AI Agent Service is, what it can do for you, and how to quickly get started via the provided Python Azure AI Foundry SDK.\nNOTE: At the time of writing, the Azure AI Agent Service is in public preview.\nOverview # The Azure Azure AI Agent Service is part of Azure AI Foundry. Azure AI Foundry is a unified AI platform that allows you to manage the whole lifecycle of your AI application. Key features, are:\nRich model catalog (OpenAI, DeepSeek, Cohere, Meta, Mistral\u0026hellip;) Deploy and experiment with different models in playgrounds Seamless Integration to other Azure services such as Azure OpenAI, Azure AI Services, and Azure AI Search Project Management with features for project creation, resource management, and access control Simplified coding experience with unified SDK Build and deploy AI agents with the Azure AI Agent Service and more\u0026hellip; With the Azure AI Agent Service, Developers can easily build extensible AI Agents using out of the box tooling and integrations into Azure services. Some of the highlights are:\nFlexible model selection (not solely OpenAI models) Knowledge tools such as Azure AI Search, Grounding with Bing Search, Microsoft Fabric and file uploads Action tools such as OpenAPI 3.0 specified tools and Code Interpreter, Azure Functions and custom functions Conversation state management (providing consistent context) and more\u0026hellip; Requirements # To start with Azure AI Agent Service, we first need to deploy the necessary Azure Services and create an Azure AI Foundry Hub and Project. Additionally, we will prepare our local development environment.\nPrepare infrastructure # In this blog post, we want to focus on how to use these services from a coding perspective. Hence, we are not going to cover the infrastructure deployment in much detail. However, we will simply use the provided bicep template from the Azure-Samples repository here.\nAfter a successful deployment, we should see the following services in your resource group.\nThe Azure AI Foundry portal can be found under https://ai.azure.com/. We should now have an Azure AI Foundry Hub and Project created for us.\nIf you want to learn more about Azure AI Foundry Hubs and Projects, visit the documentation page here.\nWe can also view the connected resources that have been created as part of the deployment and can be used for the AI agents that we want to build.\nPrepare local dev environment # For our local development environment, we need to install the following packages:\npip install python-dotenv azure-ai-projects azure-identity Next, we will use a local .env file to specify some variables to connect to our Azure AI Foundry Project.\nAZURE_AI_AGENT_PROJECT_CONNECTION_STRING=\u0026#34;your_project_connection_string\u0026#34; AZURE_AI_AGENT_MODEL_DEPLOYMENT_NAME=\u0026#34;your_model_deployment\u0026#34; Coding # If everything is running we can start coding. My goal is to build an AI Agent that acts as a Basketball Assistant Coach. For those who don\u0026rsquo;t know me, I am a big Basketball fan and combing two things that I love is pure excitement for me.\nBuilding the app # First we need to import the necessary classes from the SDK. In this case we are using the python SDK.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import CodeInterpreterTool, FilePurpose from pathlib import Path We will then load the variables from the .env file, and create the AIProjectClient and connect to our Azure AI Foundry Project.\n# load environment variables from local .env file load_dotenv() PROJECT_CONNECTION_STRING = os.getenv(\u0026#34;AZURE_AI_AGENT_PROJECT_CONNECTION_STRING\u0026#34;) MODEL_DEPLOYMENT = os.getenv(\u0026#34;AZURE_AI_AGENT_MODEL_DEPLOYMENT_NAME\u0026#34;) # create ai project client project = AIProjectClient.from_connection_string( conn_str=PROJECT_CONNECTION_STRING, credential=DefaultAzureCredential() ) with project_client: We want our agent (Assistant Coach) to be able to analyze and interpret certain Basketball statistics. In this case, we are going to upload a csv file with statistics about 3-point shots from the NBA seasons 1996 until 2020. To achieve that, we will make use of the CodeInterpreterTool.\n# upload a file and add it to the client file = project_client.agents.upload_file_and_poll( file_path=\u0026#34;nba3p.csv\u0026#34;, purpose=FilePurpose.AGENTS ) print(f\u0026#34;Uploaded file, file ID: {file.id}\u0026#34;) # create a code interpreter tool instance referencing the uploaded file code_interpreter = CodeInterpreterTool(file_ids=[file.id]) Now we can define the agent with instructions and tools.\n# create an agent agent = project_client.agents.create_agent( model=\u0026#34;gpt-4o-mini\u0026#34;, name=\u0026#34;assistant-coach-agent\u0026#34;, instructions=\u0026#34;You are a Basketball assistant coach that knows everything about the game of Basketball. You give advice about Basketball rules, training, statistics and strategies.\u0026#34;, tools=code_interpreter.definitions, tool_resources=code_interpreter.resources, ) print(f\u0026#34;Using agent: {agent.name}\u0026#34;) Next, we create a thread, which is basically the conversation between the user and the agent. In the message itself, we define the ask or task for our agent.\n# create a thread with message thread = project_client.agents.create_thread() print(f\u0026#34;Thread created: {thread.id}\u0026#34;) message = project_client.agents.create_message( thread_id=thread.id, role=\u0026#34;user\u0026#34;, content=\u0026#34;Could you please create a bar chart for 3Pointers made vs attempts during the NBA seasons 1996 until 2020 and save it as a .png file?\u0026#34;, ) Almost done, we now have to specify the run or activation part in which the agent will perform our task. Additionally, we have to fetch the output in reversed chronological order, providing a clear view of the most recent interactions first.\n# ask the agent to perform work on the thread run = project_client.agents.create_and_process_run(thread_id=thread.id, agent_id=agent.id) # fetch and print the conversation history including the last message print(\u0026#34;\\nConversation Log:\\n\u0026#34;) messages = project_client.agents.list_messages(thread_id=thread.id) for message_data in reversed(messages.data): last_message_content = message_data.content[-1] print(f\u0026#34;{message_data.role}: {last_message_content.text.value}\\n\u0026#34;) The agent should generate a graph png that we want to look at. Hence, we have to download the file with the following code snippet.\n# fetch any generated files for file_path_annotation in messages.file_path_annotations: project_client.agents.save_file(file_id=file_path_annotation.file_path.file_id, file_name=Path(file_path_annotation.text).name) print(f\u0026#34;File saved as {Path(file_path_annotation.text).name}\u0026#34;) Lastly, we should clean up and delete the agent and the thread.\n# clean up project_client.agents.delete_agent(agent.id) project_client.agents.delete_thread(thread.id) Running the app # Note: The code snippets above should just give you an overview of the Azure AI Agent Service and are not representing a production-grade application. Error handling, logging, config management and reusability are just some things you need to add to your final application code. Nevertheless, you can find the application code and the used files on my GitHub repo here.\nLet\u0026rsquo;s run our code and see the results.\n(.venv) ‚ûú basketball-ai-agent python main.py Uploaded file, file ID: assistant-J7sVvG3wVZXRkXHwSxELJY Using agent: assistant-coach-agent Thread created: thread_zqMD7857xH4zsKwN6olVvkmx Conversation Log: MessageRole.USER: Could you please create a bar chart for 3Pointers made vs attempts during the NBA seasons 1996 until 2020 and save it as a .png file? MessageRole.AGENT: Let\u0026#39;s start by examining the contents of the uploaded file to understand the data it contains. After that, we can create the bar chart for 3-pointers made versus attempts for the NBA seasons from 1996 to 2020. MessageRole.AGENT: The uploaded data contains the following columns: - **NBASeasons**: The season year. - **3PointersMade**: The number of 3-pointers made. - **3PointersAttempts**: The number of 3-pointers attempted. - **3PointersPercentage**: The percentage of successful 3-point shots. - **3PointersPercentageShareInTotalPoints**: The share of 3-pointers in total points. We\u0026#39;ll create a bar chart comparing the number of 3-pointers made versus the number of 3-pointers attempted for the NBA seasons from 1996 to 2020. Let\u0026#39;s proceed with that. MessageRole.AGENT: Here is the bar chart comparing 3-pointers made versus 3-pointers attempted from the NBA seasons 1996 to 2020. You can download it using the link below: [Download the chart](sandbox:/mnt/data/3_pointers_made_vs_attempts_1996_2020.png) File saved as 3_pointers_made_vs_attempts_1996_2020.png During the execution of our app, we can see the agent and thread getting created in Azure AI Foundry portal.\nAfter the run the agent and the thread will be deleted due to our clean-up code. If you want to keep it for troubleshooting proposes, just comment or remove the clean-up section.\nIn the Agents view we can also see the Code Interpreter got added as action tool for our agent, and we can see the file we have uploaded.\nLet\u0026rsquo;s check the results. The graph generated is correct and shows quite a raise in 3 Pointers made and attempts. Thanks Steph Curry.\nSummary # The Azure AI Agent Service allows you to quickly and with less effort build, deploy and manage AI Agents. The comprehensive out of the box toolkit and the possibility to select different models give Developers the flexibility they need to develop agent-based AI apps that can successfully accomplish complex tasks.\nThe Azure AI Agent Service can also easily be used with multi-agent frameworks such as Semantic Kernel. Stay tuned for another blog post about multi-agent apps.\nResources # AI agents ‚Äî what they are, and how they‚Äôll change the way we work Introducing Azure AI Agent Service What is Azure AI Agent Service Azure AI Agent standard deployment via bicep template from Azure AI Samples repository Azure AI Agent Code Interpreter What is Azure AI Foundry Azure AI Foundry SDK Azure AI Foundry Hubs and Projects ","date":"2 April 2025","externalUrl":null,"permalink":"/posts/agent/","section":"Posts","summary":"","title":"Intro to Azure AI Agent Service","type":"posts"},{"content":"üö®Please be aware that all content posted on this website represents only my personal opinion and is based on my experience ‚Äì it does not represent Microsoft\u0026rsquo;s positions, strategies, or opinions.üö®\n","date":"30 January 2025","externalUrl":null,"permalink":"/disclaimer/","section":"BeyondElastic","summary":"","title":"Disclaimer","type":"page"},{"content":"","date":"30 January 2025","externalUrl":null,"permalink":"/tags/intro/","section":"Tags","summary":"","title":"Intro","type":"tags"},{"content":"Hi everyone,\nWelcome to my blog! üéâ Here, I dive into the exciting world of modern application development, platform challenges, and cloud solutions. As a Technical Specialist, I\u0026rsquo;m thrilled to explore the latest trends and technologies that are shaping our industry.\nIn this space, I\u0026rsquo;ll share insights on how we can tackle today\u0026rsquo;s IT challenges using the powerful capabilities of Microsoft Azure. From cloud computing to AI-driven solutions, DevOps, and everything in between, join me on this journey to discover innovative solutions that drive efficiency and success.\nFor those who have followed my previous blog, beyondelastic.com, thank you for your continued support! I can\u0026rsquo;t wait to bring you even more valuable and fun content here. This is my first post on my new blog, and I\u0026rsquo;m excited to embark on this adventure with you all.\nStay tuned for exciting posts, and don\u0026rsquo;t hesitate to engage with your thoughts and questions!\nBest regards,\nAlex\n","date":"30 January 2025","externalUrl":null,"permalink":"/posts/intro-post/","section":"Posts","summary":"","title":"Intro Post","type":"posts"},{"content":" Welcome to my blog! My name is Alexander Ullah, and I am a proficient, hands-on, and customer-oriented technologist with 20+ years of experience and a passion for Kubernetes and cloud-native technologies, modern apps, and AI. As part of Microsoft\u0026rsquo;s Digital \u0026amp; Application Innovation team, I support our valued customers with their transformation from legacy to modern application platforms to achieve better business outcomes. In addition, I create content and speak at public conferences to inform the community about emerging technologies and the Microsoft Azure portfolio.\nYou can also find me on LinkedIn.\nPlease be aware that all posts and opinions are my own, see Disclaimer\n","date":"30 January 2025","externalUrl":null,"permalink":"/author/","section":"BeyondElastic","summary":"","title":"Author","type":"page"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]